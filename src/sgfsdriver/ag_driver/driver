#!/usr/bin/env python

"""
   Copyright 2016 The Trustees of University of Arizona

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"""

"""
Filesystem AG driver.
Serves files on a remote system through generic fs plugin.
"""

import traceback
import sys
import errno
import threading
import Queue
import json
import syndicate.util.gateway as gateway
import sgfsdriver.lib.abstractfs as abstractfs

from sgfsdriver.lib.pluginloader import pluginloader
from expiringdict import ExpiringDict

DEFAULT_READ_TTL = 60 * 5     # 5min
DEFAULT_WRITE_TTL = 60 * 5     # 5min
DEFAULT_DATA_CACHE_SIZE = 300      # 300 blocks
DEFAULT_DATA_CACHE_TTL = 60 * 5     # 5min

fs = None
storage_dir = None
sync_on_init = True
follow_symlink = True
read_ttl = DEFAULT_READ_TTL * 1000
write_ttl = DEFAULT_WRITE_TTL * 1000
data_cache_size = DEFAULT_DATA_CACHE_SIZE
data_cache_ttl = DEFAULT_DATA_CACHE_TTL

# will store commands to be processed
command_queue = Queue.Queue(0)

# data cache
data_cache = None

def _initFS(driver_config, driver_secrets, role):
    global fs
    global storage_dir
    global sync_on_init
    global follow_symlink
    global read_ttl
    global write_ttl
    global data_cache_size
    global data_cache_ttl
    global data_cache

    gateway.log_debug("_initFS")

    if fs:
        return True

    # continue only when fs is not initialized
    if "DRIVER_FS_PLUGIN" not in driver_config:
        gateway.log_error("No DRIVER_FS_PLUGIN defined")
        return False

    if "DRIVER_FS_PLUGIN_CONFIG" not in driver_config:
        gateway.log_error("No DRIVER_FS_PLUGIN_CONFIG defined")
        return False

    if "DATASET_DIR" not in driver_config:
        gateway.log_error("No DATASET_DIR defined")
        return False

    storage_dir = driver_config['DATASET_DIR']
    storage_dir = "/" + storage_dir.strip("/")

    if "SYNC_ON_INIT" in driver_config:
        sync_on_init = bool(driver_config["SYNC_ON_INIT"])

    if "FOLLOW_SYMLINK" in driver_config:
        follow_symlink = bool(driver_config["FOLLOW_SYMLINK"])

    if "MS_TTL" in driver_config:
        read_ttl = int(driver_config["MS_TTL"]) * 1000

    if "AG_TTL" in driver_config:
        write_ttl = int(driver_config["AG_TTL"]) * 1000

    if "DATA_CACHE_SIZE" in driver_config:
        data_cache_size = int(driver_config["DATA_CACHE_SIZE"])

    if "DATA_CACHE_TTL" in driver_config:
        data_cache_ttl = int(driver_config["DATA_CACHE_TTL"])

    plugin = driver_config["DRIVER_FS_PLUGIN"]

    if isinstance(driver_config["DRIVER_FS_PLUGIN_CONFIG"], dict):
        plugin_config = driver_config["DRIVER_FS_PLUGIN_CONFIG"]
    elif isinstance(driver_config["DRIVER_FS_PLUGIN_CONFIG"], basestring):
        json_plugin_config = driver_config["DRIVER_FS_PLUGIN_CONFIG"]
        plugin_config = json.loads(json_plugin_config)

    plugin_config["secrets"] = driver_secrets
    plugin_config["work_root"] = storage_dir

    data_cache = ExpiringDict(
        max_len=data_cache_size,
        max_age_seconds=data_cache_ttl
    )

    try:
        loader = pluginloader()
        fs = loader.load(plugin, plugin_config, role)

        if not fs:
            gateway.log_error("No such driver plugin found: %s" % plugin)
            return False

        if abstractfs.afsgateway.AG not in fs.get_supported_gateways():
            gateway.log_error(
                "The plugin does not support AG: %s" %
                fs.get_supported_gateways()
            )
            return False

        fs.set_notification_cb(datasets_update_cb)
        fs.connect()
    except Exception as e:
        gateway.log_error("Unable to initialize a driver")
        gateway.log_error(str(e))
        traceback.print_exc()
        return False

    gateway.log_debug("Driver initialized")
    return True


def _shutdownFS():
    global fs

    gateway.log_debug("_shutdownFS")

    if fs:
        try:
            fs.close()
        except Exception:
            pass
    fs = None


def _get_data_block(file_system, file_path, byte_offset, byte_len):
    key = "%s|%d|%d" % (file_path, byte_offset, byte_len)
    if key in data_cache:
        return data_cache[key]

    try:
        buf = file_system.read(file_path, byte_offset, byte_len)
        data_cache[key] = buf
        return buf
    except Exception, e:
        raise IOError("Failed to read %s: %s" % (file_path, e))


def _invalidate_data_blocks(file_path):
    finding_key = "%s|" % (file_path)
    to_be_deleted_keys = []
    for key in data_cache:
        if key.startswith(finding_key):
            to_be_deleted_keys.append(key)

    for key in to_be_deleted_keys:
        del data_cache[key]


def datasets_update_cb(updated_entries, added_entries, removed_entries):
    gateway.log_debug("datasets_update_cb")

    for u in updated_entries:
        if u.stat and not u.stat.directory:
            # file
            _invalidate_data_blocks(u.path)

            cmd = gateway.make_metadata_command(
                "put",
                "file",
                0755,
                u.stat.size,
                u.path,
                read_ttl=read_ttl,
                write_ttl=write_ttl
            )
            gateway.log_debug("Queuing a command %s" % cmd['path'])
            command_queue.put((cmd, None))
        else:
            # directory
            cmd = gateway.make_metadata_command(
                "put",
                "directory",
                0755,
                None,
                u.path,
                read_ttl=read_ttl,
                write_ttl=write_ttl
            )
            gateway.log_debug("Queuing a command %s" % cmd['path'])
            command_queue.put((cmd, None))

    for a in added_entries:
        if a.stat and not a.stat.directory:
            # file
            _invalidate_data_blocks(a.path)

            cmd = gateway.make_metadata_command(
                "put",
                "file",
                0755,
                a.stat.size,
                a.path,
                read_ttl=read_ttl,
                write_ttl=write_ttl
            )
            gateway.log_debug("Queuing a command %s" % cmd['path'])
            command_queue.put((cmd, None))
        else:
            # directory
            cmd = gateway.make_metadata_command(
                "put",
                "directory",
                0755,
                None,
                a.path,
                read_ttl=read_ttl,
                write_ttl=write_ttl
            )
            gateway.log_debug("Queuing a command %s" % cmd['path'])
            command_queue.put((cmd, None))

    for r in removed_entries:
        _invalidate_data_blocks(r.path)

        cmd = gateway.make_metadata_delete_command(r.path)
        gateway.log_debug("Queuing a command %s" % cmd['path'])
        command_queue.put((cmd, None))


def _resync(path):
    gateway.log_debug("_resync")

    stack = [path]
    while len(stack) > 0:
        last_dir = stack.pop(0)
        fs.clear_cache(last_dir)
        entries = fs.list_dir(last_dir)
        if entries:
            for entry in entries:
                # entry is a filename
                entry_path = last_dir.rstrip("/") + "/" + entry
                st = fs.stat(entry_path)
                if st:
                    e = abstractfs.afsevent(entry_path, st)

                    if st.directory:
                        # do sync recursively
                        if (st.symlink and follow_symlink) or (not st.symlink):
                            stack.append(entry_path)

                    datasets_update_cb([], [e], [])


def driver_init(driver_config, driver_secrets):
    """
    Do the one-time driver setup.
    """
    gateway.log_debug("driver_init")

    # detect a role
    rolestr = sys.argv[1]
    role = abstractfs.afsrole.DISCOVER
    if rolestr == "read":
        role = abstractfs.afsrole.READ
    elif rolestr == "crawl":
        role = abstractfs.afsrole.DISCOVER
    else:
        gateway.log_error("Unknown role: %s" % rolestr)
        return False

    if not _initFS(driver_config, driver_secrets, role):
        gateway.log_error("Unable to init filesystem")
        return False

    if not fs.exists("/"):
        gateway.log_error("No such file or directory: %s" % storage_dir)
        return False

    if not fs.is_dir("/"):
        gateway.log_error("Not a directory: %s" % storage_dir)
        return False

    if role == abstractfs.afsrole.DISCOVER:
        if sync_on_init:
            # add initial dataset
            _resync("/")

    return True


def driver_shutdown():
    """
    Do the one-time driver shutdown
    """
    gateway.log_debug("driver_shutdown")

    _shutdownFS()


def next_dataset(driver_config, driver_secrets):
    """
    Return the next dataset command for the AG to process.
    Should block until there's data.

    Must call gateway.crawl() to feed the data into the AG.
    Return True if there are more datasets to process.
    Return False if not.
    """
    gateway.log_debug("next_dataset")

    # find the next command
    while True:
        # this will block if command is not immediately available
        cmd_sem = command_queue.get(True)
        cmd = cmd_sem[0]
        sem = cmd_sem[1]
        gateway.log_debug("Processing a new command %s" % cmd['path'])

        if cmd is not None:
            # send the command to the AG and get back the result
            rc = gateway.crawl(cmd)
            if rc != 0:
                gateway.log_error("Failed to crawl %s" % cmd['path'])

            command_queue.task_done()
            if sem:
                sem.release()
            gateway.log_debug("Processed a command %s" % cmd['path'])

            # have more data - wait for next commands
            return True
        else:
            # try next path
            gateway.log_error("Could not fetch the command")
            continue

    return False


def refresh(request, driver_config, driver_secrets):
    """
    Request to refresh a particular path.
    Verify that it still exists, and if so,
    queue it for republishing.
    """
    gateway.log_debug("refresh")

    path = gateway.request_path(request)
    file_path = gateway.path_join("/", path)

    fs.clear_cache(file_path)
    cmd = None
    sem = threading.Semaphore()
    sem.acquire()

    if not fs.exists(file_path):
        # delete
        gateway.log_debug("No longer present: '%s'" % file_path)
        cmd = gateway.make_metadata_delete_command(file_path)
        gateway.log_debug("Queuing a command %s" % cmd['path'])
        command_queue.put((cmd, sem))

        _invalidate_data_blocks(file_path)
    else:
        # update
        gateway.log_debug("Still present: '%s'" % file_path)
        stat = fs.stat(file_path)
        if stat:
            if not stat.directory:
                # file
                cmd = gateway.make_metadata_command(
                    "put",
                    "file",
                    0755,
                    stat.size,
                    file_path,
                    read_ttl=read_ttl,
                    write_ttl=write_ttl
                )
                gateway.log_debug("Queuing a command %s" % cmd['path'])
                command_queue.put((cmd, sem))
            else:
                # directory
                cmd = gateway.make_metadata_command(
                    "put",
                    "directory",
                    0755,
                    None,
                    file_path,
                    read_ttl=read_ttl,
                    write_ttl=write_ttl
                )
                gateway.log_debug("Queuing a command %s" % cmd['path'])
                command_queue.put((cmd, sem))

    # need to block until the queued command is processed
    gateway.log_debug("Waiting for a new command to be processed")
    # wait for semaphore release by next_dataset
    sem.acquire()
    # do nothing
    sem.release()
    #command_queue.join()

    return 0


def read(request, chunk_fd, driver_config, driver_secrets):
    """
    Read a chunk of data.
    @request is a DriverRequest
    @chunk_fd is a file descriptor to which to write the data.
    @driver_config is a dict containing the driver's config
    @driver_secrets is a dict containing the driver's unencrypted secrets
    """
    gateway.log_debug("read")

    path = gateway.request_path(request)
    file_path = gateway.path_join("/", path)
    byte_offset = gateway.request_byte_offset(request)
    byte_len = gateway.request_byte_len(request)

    if byte_offset is None:
        # this is a bug
        gateway.log_error(
            "BUG: byte offset of request on %s is not defined" % file_path
        )
        sys.exit(1)

    if byte_len is None:
        # this is a bug
        gateway.log_error(
            "BUG: byte len of request on %s is not defined" % file_path
        )
        sys.exit(1)

    if not fs.exists(file_path):
        gateway.log_error("No such file or directory: %s" % file_path)
        return -errno.ENOENT

    # read
    try:
        buf = _get_data_block(fs, file_path, byte_offset, byte_len)
    except Exception, e:
        gateway.log_error("Failed to read %s: %s" % (file_path, e))
        return -errno.EREMOTEIO

    # send it off
    chunk_fd.write(buf)
    return 0
